{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "import re, string, unicodedata\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import time\n",
    "nltk.download('stopwords')\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 도서 text SVM\n",
    "\n",
    "- 도서 설명 text 불러오기\n",
    "- 각 도서 text에서 감정 추출 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**도서 text 텍스트 전처리**\n",
    "\n",
    "- 텍스트 전처리 함수에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hapus_url(text):\n",
    "    mention_pattern = r'@[\\w]+'\n",
    "    cleaned_text = re.sub(mention_pattern, '', text)\n",
    "    return re.sub(r'http\\S+','', cleaned_text)\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-Z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "def stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text=' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "def final_clean(text):\n",
    "    final_text= []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in sw and i.strip().lower().isalpha():\n",
    "            final_text.append(i.strip().lower())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "def clean(text):\n",
    "    text = hapus_url(text)\n",
    "    text = remove_special_characters(text, remove_digits=True)\n",
    "    text = stemmer(text)\n",
    "    text = final_clean(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 도서 text\n",
    "text = '''끔찍한 실수로 아내를 죽이고 벽 안에 묻어버리는 주인공, 그런데 한때 자신이 애지중지한 고양이를 함께 가둬버리는 바람에 사람들에게 발각된다는 유명한 스토리를 한 번쯤 들어보았을 것이다. 쇠락해 가는 가문의 마지막 후계자로 고택과 함께 스러져 내리는 남매, 그저 자신을 바라보는 텅 빈 눈동자를 참을 수 없어 이웃집 노인을 살해하는 한 남자의 이야기는 어떤가? 에드거 앨런 포의 작품 세계는 어둡고 음침하다. 그러나 놀랍도록 세밀하고 탁월하다. 「검은 고양이」 「어셔가의 붕괴」 「고자질하는 심장」 등 공포 문학의 독보적인 선구자 에드거 앨런 포는 인간의 병약한 신경과 무질서한 정신세계를 치밀한 구조와 정교한 짜임새로 그린다. 그래서 기묘하고 충격적이면서도 어쩐지 포의 이야기는 읽는 이의 이해와 몰입을 불러일으킨다. 한편 포의 천재성은 최초의 탐정 소설이자 셜록 홈즈를 탄생시킨 「모르그가의 살인 사건」과 「도둑맞은 편지」 등 미스터리·추리소설에서도 빛을 발한다. 시초격이라고 할 수 있는 장르 문학에서조차 200년이 지난 현대극에서도 상상할 수 없는 반전이 도사리고 있다. 그런가 하면 풍자 소설에서는 재치와 기지가 돋보여서 「일주일에 세 번의 일요일」 「정확한 과학 중 하나로 여겨지는 사기술」 「타르 박사와 페더 교수의 치료법」을 읽으면서는 포의 기발한 상상과 익살에 웃음을 터트리게 된다. 포의 천재성은 굳이 말로 설명하지 않아도 그의 작품에 여실히 드러난다. 인생의 희극과 비극을 치열하게 겪은 작가, 그만큼 인간의 양면성과 기이한 심리를 섬세하게 포착한 작가, 정밀한 구도와 섬세한 필체로 아낌없이 풀어낸 그의 세계를 이 열 편의 엄선한 작품선집으로 담았다. 독자들에게 아름답고도 비극적이며 소름끼치는 광기의 미학을 선사할 것이다.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\82109\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "translator = Translator()\n",
    "time.sleep(1)\n",
    "text_t = translator.translate(text)\n",
    "text_e = text_t.text\n",
    "text_e = clean(text_e)\n",
    "text_df = pd.DataFrame([text_e], columns = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    main charact kill hi wife buri wall terribl mi...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**도서 text에 SVM 모델 적용**\n",
    "\n",
    "- 입력한 도서의 text 감정 벡터 추출 결과 = `book_sentiment`\n",
    "- SVM 모델이 트위터 데이터로 학습했기 때문에 학습 당시 사용했던 input 형태와 동일한 형태로 현재 가사 text input을 변형해야함\n",
    "- tf-idf 모델을 트위터 데이터로 fit_transform, 가사 text 데이터에 transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'text 감정 추출 모델'\n",
    "df = pd.read_csv(path + '/tweet_data_agumentation.csv', index_col = 0)\n",
    "\n",
    "model = joblib.load(path + '/SVM.pkl')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00793652, 0.32936839, 0.01008081, 0.46980171, 0.08457674,\n",
       "        0.01568081, 0.02345192, 0.04166963, 0.000539  , 0.01640461,\n",
       "        0.00048987]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df로 tf-idf 학습\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit_transform(df[\"content\"])\n",
    "\n",
    "# 학습한 tf-idf 모델을 도서 text에 동일하게 적용\n",
    "text2 = tfidf_vect.transform(text_df['text'])\n",
    "model.predict_proba(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>감정</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.469802</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.329368</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.084577</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.041670</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.023452</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.016405</td>\n",
       "      <td>relief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.015681</td>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010081</td>\n",
       "      <td>enthusiasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007937</td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000539</td>\n",
       "      <td>boredom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000490</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        prob          감정\n",
       "3   0.469802       worry\n",
       "1   0.329368     sadness\n",
       "4   0.084577        love\n",
       "7   0.041670   happiness\n",
       "6   0.023452        hate\n",
       "9   0.016405      relief\n",
       "5   0.015681         fun\n",
       "2   0.010081  enthusiasm\n",
       "0   0.007937       empty\n",
       "8   0.000539     boredom\n",
       "10  0.000490       anger"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_sentiment = pd.DataFrame(model.predict_proba(text2), index=['prob']).T\n",
    "book_sentiment['감정'] = ['empty','sadness','enthusiasm','worry','love','fun','hate','happiness','boredom','relief','anger']\n",
    "\n",
    "book_sentiment = book_sentiment.sort_values(by='prob',ascending=False)\n",
    "book_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
